# Input Output Hidden Markov Models

The Input/Output Hidden Markov Model (IOHMM) is an alternative architecture proposed by @bengio1995input to learn to map input sequences to output sequences.

IOHMM can be seen as a recurrent version of a mixture of experts (cite?).

HMM is unsupervised. HMM learns the output sequence distribution but not the output sequence itself.

Although effective for learning short  term  memories, many of these alternatives suffer from practical difficulties when the input/output sequences span long points.

general sequence processing tasks, such as production, classification, or prediction, can be dealt with

## Model specification
As with HMM, IOHMM involves two interconnected models. 
\[
z_{t} = f(z_{t-1}, \mat{u}_{t}) \\
\mat{y}_{t} = g(z_{t  }, \mat{u}_{t})
\]

The first line corresponds to the state model, which consists of a discrete-time, discrete-state Markov chain with hidden states $z_t \in \{1, \dots, K\}$ that transition according to $f(z_t | z_{t-1}, \mat{u}_{t})$. Additionally, the observation model is governed by $g(z_{t  }, \mat{u}_{t})$, where $\mat{u}_{t} \in \RR^m$ is the input vector and $\mat{x}_t \in \RR^r$ is the vector of observations, emissions or output. The corresponding joint distribution, 

\[
p(\mat{z}_{1:T}, \mat{x}_{1:T} | \mat{u}_{t}),
\]

  <!-- = p(\mat{z}_{1:T}) p(\mat{x}_{1:T} | \mat{z}_{1:T}) -->
  <!-- = \left[ p(z_1) \prod_{t=2}^{T}{p(z_t | z_{t-1})} \right] \left[ \prod_{t=1}^{T}{p(\mat{x}_t | z_{t})} \right]. -->

depend on the parametrization. A possible parametrization for continuous inputs and outputs models the transition matrix by means of a logistic regression model whose parameters depend on the previos hidden state

\[
p(z_t | \mat{x}_{t}, \mat{u}_{t}, z_{t-1} = i) = \text{logit}^{-1}(\mat{\beta}_i \mat{u}_{t})),
\]

and the observations with a Gaussian density with parameters depending on the current state

\[
p(\mat{x}_t | \mat{u}_{t}, z_{t} = j) = \mathcal{N}(\mat{x}_t | \mat{W}_j \mat{u}_t, \mat{\Sigma}_j.
\]

Given this new probabilistic model, state variables are assumed to follow a multinomial distribution depending on the input variable. The belief states now becomes

\begin{align*}
\alpha_t(j) 
  & \triangleq  p(z_t = j | \mat{x}_{1:t}, \mat{u}_{1:t}). \\
\end{align*}

The transition probabilities $\Psi_t(i, j) = p(z_t = j | z_{t-1} = i, \mat{u}_{1:t-1})$, which govern the state dynamics, now depend on the input sequence. The output $\mat{eta}_t = \ev{\mat{x}_t | z_t, \mat{u}_t}$ can be interpreted as the expected location parameter for the probability distribution of the emission $\mat{x}$_{t}, conditional on the input vector $\mat{u}_t$ and the hidden state $z_t$. The actual form of the emission density $f_{\mat{x}}(\mat{x}_t, \mat{eta}_t)$ can be discrete or continuous. In case of sequence classification or symbolic mutually exclusive emissions, a multinomial distribution can be achieved by application the softmax function to the outputs. If the chosen density is Gaussian, then the target is a linear combination of the output.

Following a probabilistic logic, the inputs and state distributions at a given point are used to estimate the state distribution and the output distribution for the next step.

In this context, the local evidence at time $t$ becomes $\psi_t(j) = p(\mat{x}_t | z_t = j, \mat{eta}_t)$, where $\mat{eta}_t = \ev{\mat{x}_t | z_t, \mat{u}_t}$ can be interpreted as the expected location parameter for the probability distribution of the emission $\mat{x}$_{t}, conditional on the input vector $\mat{u}_t$ and the hidden state $z_t$. Similarly, the transition probability becomes $\Psi(i, j) = p(z_t = j | z_{t-1} = i)$.

### Filtering
Filtered maginals can be computed recursively by adjusting the forward algorithm to consider the input sequence

\begin{align*}
\alpha_t(j) 
  & \triangleq  p(z_t = j | \mat{x}_{1:t}, \mat{u}_{1:t}) \\
  & = \sum_{l = 1}^{K}{p(z_t = j | z_{t-1} = i, \mat{u}_{t}) p(z_{t-1} = i | \mat{u}_{1:t-1})} \\
  & = \sum_{l = 1}^{K}{\Psi_t(i, j) p(z_{t-1} = i | \mat{u}_{1:t-1})} \\
  & = \psi_t(j) \sum_{l}{\Psi_t(i, j) \alpha_{t-1}(l)}.
\end{align*}

### Smoothing
Similarly, inference about the smoothed posterior marginal can be computed adjusting the forwards-backwards algorithm to consider the input sequence in both components $\alpha_t(j)$ and $\beta_t(j)$. The future component becomes

\begin{align*}
\beta_{t-1}(i)
  &= \psi_t(j) \sum_{l}{\Psi_{t}(i, j) \beta_t(l)}
\end{align*}

