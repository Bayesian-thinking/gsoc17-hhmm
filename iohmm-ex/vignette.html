<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Luis Damiano, Brian Peterson, Michael Weylandt" />

<meta name="date" content="2017-06-11" />

<title>Input-Output Hidden Markov Model</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>



<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">Input-Output Hidden Markov Model</h1>
<h4 class="author"><em>Luis Damiano, Brian Peterson, Michael Weylandt</em></h4>
<h4 class="date"><em>2017-06-11</em></h4>



<p>This work aims at replicating the Input-Output Hidden Markov Model (IOHMM) originally proposed by Hassan (2005) to forecast stock prices. The main goal is to produce public programming code in <a href="http://mc-stan.org/">Stan</a> for a fully bayesian estimation of the model parameters and inferene on hidden quantities (including filtered state belief, smoothed state belief and jointly most likely state path). The model is introduced briefly, a deeper mathematical treatment can be found in our <a href="../litreview/main.pdf">literature review</a>.</p>
<div id="acknowledgements" class="section level3">
<h3>Acknowledgements</h3>
<p>The authors acknowledge Google for financial support via the Google Summer of Code 2017.</p>
<hr />
</div>
<div id="the-input-output-hidden-markov-model" class="section level1">
<h1>The Input-Output Hidden Markov Model</h1>
<p>The IOHMM is an architecture proposed by <span class="citation">Bengio and Frasconi (1995)</span> to map input sequences, sometimes called the control signal, to output sequences. It differs from HMM, which is part of the unsupervised learning paradigm, since it is capable of learning the output sequence itself instead of just the output sequence distribution. IOHMM is a probabilistic framework that can deal with general sequence processing tasks such as production, classification, or prediction.</p>
<div id="definitions" class="section level2">
<h2>Definitions</h2>
<p>As with HMM, IOHMM involves two interconnected models,</p>
<span class="math display">\[\begin{align*}
z_{t} &amp;= f(z_{t-1}, \mat{u}_{t}) \\
\mat{x}_{t} &amp;= g(z_{t  }, \mat{u}_{t}).
\end{align*}\]</span>
<p>The first line corresponds to the state model, which consists of discrete-time, discrete-state hidden states <span class="math inline">\(z_t \in \{1, \dots, K\}\)</span> whose transition depends on the previous hidden state <span class="math inline">\(z_{t-1}\)</span> and the input vector <span class="math inline">\(\mat{u}_{t} \in \RR^M\)</span>. Additionally, the observation model is governed by <span class="math inline">\(g(z_{t}, \mat{u}_{t})\)</span>, where <span class="math inline">\(\mat{x}_t \in \RR^R\)</span> is the vector of observations, emissions or output. The corresponding joint distribution,</p>
<p><span class="math display">\[
p(\mat{z}_{1:T}, \mat{x}_{1:T} | \mat{u}_{t}),
\]</span></p>
<p>In this parametrization for continuous inputs and outputs, the state model involves a multinomial regression whose parameters depend on the previous state <span class="math inline">\(i\)</span>,</p>
<p><span class="math display">\[
p(z_t | \mat{x}_{t}, \mat{u}_{t}, z_{t-1} = i) = \text{softmax}^{-1}(\mat{w}_i \mat{u}_{t}),
\]</span></p>
<p>and the observation model is built upon the Gaussian density with parameters depending on the current state <span class="math inline">\(j\)</span>,</p>
<p><span class="math display">\[
p(\mat{x}_t | \mat{u}_{t}, z_{t} = j) = \mathcal{N}(\mat{x}_t | \mat{b}_j \mat{u}_t, \mat{\Sigma}_j).
\]</span></p>
<p>IOHMM adapts the logics of HMM to allow for input and output vectors, retaining its fully probabilistic quality. Hidden states are assumed to follow a multinomial distribution that depends on the input sequence. The transition probabilities <span class="math inline">\(\Psi_t(i, j) = p(z_t = j | z_{t-1} = i, \mat{u}_{t})\)</span>, which govern the state dynamics, are driven by the control signal as well.</p>
<p>As for the output sequence, the local evidence at time <span class="math inline">\(t\)</span> now becomes <span class="math inline">\(\psi_t(j) = p(\mat{x}_t | z_t = j, \mat{\eta}_t)\)</span>, where <span class="math inline">\(\mat{\eta}_t = \ev{\mat{x}_t | z_t, \mat{u}_t}\)</span> can be interpreted as the expected location parameter for the probability distribution of the emission <span class="math inline">\(\mat{x}_{t}\)</span> conditional on the input vector <span class="math inline">\(\mat{u}_t\)</span> and the hidden state <span class="math inline">\(z_t\)</span>.</p>
</div>
<div id="inference" class="section level2">
<h2>Inference</h2>
<p>There are several quantities of interest to be inferred from different algorithms. In this section, the discussion assumes that model parameters <span class="math inline">\(\mat{\theta}\)</span> are known.</p>
<div id="filtering" class="section level3">
<h3>Filtering</h3>
<p>A filter infers the belief state at a given step based on all the information available up to that point,</p>
<span class="math display">\[\begin{align*}
\alpha_t(j)
  &amp; \triangleq p(z_t = j | \mat{x}_{1:t}, \mat{u}_{1:t}).
\end{align*}\]</span>
<p>It achieves better noise reduction than simply estimating the hidden state based on the current estimate <span class="math inline">\(p(z_t | \mat{x}_{t})\)</span>. The filtering process can be run online, or recursively, as new data streams in. Filtered maginals can be computed recursively by means of the forward algorithm <span class="citation">(Baum and Eagon 1967)</span>.</p>
</div>
<div id="smoothing" class="section level3">
<h3>Smoothing</h3>
<p>A smoother infers the belief state at a given state based on all the observations or evidence,</p>
<p><span class="math display">\[
\begin{align*}
\gamma_t(j)
  &amp; \triangleq p(z_t = j | \mat{x}_{1:T}, \mat{u}_{1:T}) \\
  &amp; \propto \alpha_t(j) \beta_t(j),
\end{align*}
\]</span></p>
<p>where</p>
<span class="math display">\[\begin{align*}
\beta_{t-1}(i)
  &amp; \triangleq p(\mat{x}_{t:T} | z_{t-1} = i, \mat{u}_{t:T}).
\end{align*}\]</span>
<p>Although noise and uncertainty are significantly reduced as a result of conditioning on past and future data, the smoothing process can only be run offline. Inference can be done by means of the forwards-backwards algorithm, also know as the Baum-Welch algorithm <span class="citation">(Baum and Eagon 1967, <span class="citation">Baum et al. (1970)</span>)</span>. Let <span class="math inline">\(\gamma_t(j)\)</span> be the desired smoothed posterior marginal,</p>
<p>It is also of interest to compute the most probable state sequence or path,</p>
<p><span class="math display">\[
\mat{z}^* = \argmax_{\mat{z}_{1:T}} p(\mat{z}_{1:T} | \mat{x}_{1:T}).
\]</span></p>
<p>The jointly most probable sequence of states can be inferred by means of maximum a posterior (MAP) estimation or Viterbi decoding.</p>
</div>
</div>
<div id="parameter-estimation" class="section level2">
<h2>Parameter estimation</h2>
<p>The parameters of the models are <span class="math inline">\(\mat{\theta} = (\mat{\pi}_1, \mat{\theta}_h, \mat{\theta}_o)\)</span>, where <span class="math inline">\(\mat{\pi}_1\)</span> is the initial state distribution, <span class="math inline">\(\mat{\theta}_h\)</span> are the parameters of the hidden model and <span class="math inline">\(\mat{\theta}_o\)</span> are the parameters of the state-conditional density function <span class="math inline">\(p(\mat{x}_t | z_t = j, \mat{u}_t)\)</span>. The form of <span class="math inline">\(\mat{\theta}_h\)</span> and <span class="math inline">\(\mat{\theta}_o\)</span> depend on the specification of the model. State transition may be characterized by a logistic or multinomial regression with parameters <span class="math inline">\(\mat{w}_k\)</span> for <span class="math inline">\(k \in \{1, \dots, K\}\)</span>, while emissions may be modelled with with a linear regression with Gaussian error with parameters <span class="math inline">\(\mat{b}_k\)</span> and <span class="math inline">\(\mat{\Sigma}_k\)</span> for <span class="math inline">\(k \in \{1, \dots, K\}\)</span>.</p>
<hr />
</div>
</div>
<div id="learning-by-simulation" class="section level1">
<h1>Learning by simulation</h1>
<p>We approach the model with simulations before diving into real data. We first create a simulation routine to generate data that complies with the model assumptions and we then write a MCMC sampler to recover the parameters. After that, we finally move into analysing real data from actual stock markets as proposed in the original work.</p>
<p>This approach has many benefits, including:</p>
<ul>
<li>We can confirm that the sampler works as intended, i.e. it retrieves the parameters used in the data simulation stage.</li>
<li>The generated data meets all the assumptions and the estimates are free of the effects of data contamination, thus simplifying the interpretation of the results.</li>
<li>The user can gain insight into the model functioning by trying different combinations of the parameters and/or inputs.</li>
</ul>
<div id="generative-model" class="section level2">
<h2>Generative model</h2>
<p>We first write an R function for our generative model. The inputs are the sequence length <span class="math inline">\(T\)</span>, the number of discrete hidden states <span class="math inline">\(K\)</span>, the input matrix <span class="math inline">\(\mat{u}\)</span>, the initial state distribution vector <span class="math inline">\(\mat{\pi}_1\)</span>, and finally the parameters from both the hidden states and the observation models <span class="math inline">\(\mat{\theta}_h\)</span> and <span class="math inline">\(\mat{\theta}_o\)</span> respectively.</p>
<p>The hidden state for the first step is drawn from a multinomial according to the initial state probability vector. The transition probabilities for each of the following steps <span class="math inline">\(t\)</span> are computed from a multinomial linear regression with vector parameters <span class="math inline">\(\mat{w}_k\)</span>, one set per possible hidden state <span class="math inline">\(k = 1, \dots, K\)</span>, and covariates <span class="math inline">\(\mat{u}_t\)</span>. The hidden states are subsequently drawn from the transition probabilities at each step.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">iohmm_sim &lt;-<span class="st"> </span>function(T, K, u, p.init, w, obs.model, b, S) {
  m &lt;-<span class="st"> </span><span class="kw">ncol</span>(u)
  <span class="co"># r &lt;- mcol(b)</span>

  if (<span class="kw">dim</span>(u)[<span class="dv">1</span>] !=<span class="st"> </span>T)
    <span class="kw">stop</span>(<span class="st">&quot;The input matrix must have T rows.&quot;</span>)

  if (<span class="kw">any</span>(<span class="kw">dim</span>(w) !=<span class="st"> </span><span class="kw">c</span>(K, m)))
    <span class="kw">stop</span>(<span class="st">&quot;The transition weight matrix must be of size Kxm, where m is the size of the input vector.&quot;</span>)

  if (<span class="kw">any</span>(<span class="kw">dim</span>(b) !=<span class="st"> </span><span class="kw">c</span>(K, m)))
    <span class="kw">stop</span>(<span class="st">&quot;The regressors matrix must be of size Kxm, where m is the size of the input vector.&quot;</span>)

  if (<span class="kw">length</span>(p.init) !=<span class="st"> </span>K)
    <span class="kw">stop</span>(<span class="st">&quot;The vector p.init must have length K.&quot;</span>)

  p.mat &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow =</span> T, <span class="dt">ncol =</span> K)
  p.mat[<span class="dv">1</span>, ] &lt;-<span class="st"> </span>p.init

  z &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="st">&quot;numeric&quot;</span>, T)
  z[<span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> <span class="dv">1</span>:K, <span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">replace =</span> <span class="ot">FALSE</span>, <span class="dt">prob =</span> p.init)
  for (t in <span class="dv">2</span>:T) {
    p.mat[t, ] &lt;-<span class="st"> </span><span class="kw">softmax</span>(<span class="kw">sapply</span>(<span class="dv">1</span>:K, function(j) {u[t, ] %*%<span class="st"> </span>w[j, ]}))
    z[t] &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> <span class="dv">1</span>:K, <span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">replace =</span> <span class="ot">FALSE</span>, <span class="dt">prob =</span> p.mat[t, ])
  }

  x &lt;-<span class="st"> </span><span class="kw">do.call</span>(obs.model, <span class="kw">list</span>(u, z, b, S))

  <span class="kw">list</span>(
    <span class="dt">u =</span> u,
    <span class="dt">z =</span> z,
    <span class="dt">x =</span> x,
    <span class="dt">p.mat =</span> p.mat
  )
}</code></pre></div>
<p>The observations for each step are generated from a linear regressions with parameters <span class="math inline">\(\mat{b}_k\)</span>, …</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">obs.model &lt;-<span class="st"> </span>function(u, z, b, s) {
  T.length &lt;-<span class="st"> </span><span class="kw">nrow</span>(u)

  x &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="st">&quot;numeric&quot;</span>, T.length)
  for (t in <span class="dv">1</span>:T.length) {
    x[t] &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>, u[t, ] %*%<span class="st"> </span>b[z[t], ], s[z[t]])
  }
  <span class="kw">return</span>(x)
}</code></pre></div>
<hr />
</div>
</div>
<div id="stock-market-forecasting-using-hidden-markov-model" class="section level1">
<h1>Stock Market Forecasting Using Hidden Markov Model</h1>
<p>(the replication with actual data)</p>
<div id="refs" class="references">
<div id="ref-baum1967inequality">
<p>Baum, Leonard E., and J. A. Eagon. 1967. “An Inequality with Applications to Statistical Estimation for Probabilistic Functions of Markov Processes and to a Model for Ecology.” <em>Bulletin of the American Mathematical Society</em> 73 (3). American Mathematical Society (AMS): 360–64. doi:<a href="https://doi.org/10.1090/s0002-9904-1967-11751-8">10.1090/s0002-9904-1967-11751-8</a>.</p>
</div>
<div id="ref-baum1970maximization">
<p>Baum, Leonard E., Ted Petrie, George Soules, and Norman Weiss. 1970. “A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains.” <em>The Annals of Mathematical Statistics</em> 41 (1). Institute of Mathematical Statistics: 164–71. doi:<a href="https://doi.org/10.1214/aoms/1177697196">10.1214/aoms/1177697196</a>.</p>
</div>
<div id="ref-bengio1995input">
<p>Bengio, Yoshua, and Paolo Frasconi. 1995. “An Input Output Hmm Architecture.”</p>
</div>
</div>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
