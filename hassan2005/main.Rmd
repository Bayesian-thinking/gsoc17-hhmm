---
title: "Input-Output Hidden Markov Model"
author: "Luis Damiano, Brian Peterson, Michael Weylandt"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    number_sections: true
    self_contained: true
    includes:
      in_header: ../common/Rmd/preamble-html.Rmd
vignette: >
  %\VignetteIndexEntry{Input-Output Hidden Markov Model}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: ../common/references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

This work aims at replicating the Input-Output Hidden Markov Model (IOHMM) originally proposed by @hassan2005stock to forecast stock prices. The main goal is to produce public programming code in [Stan](http://mc-stan.org/) [@carpenter2016stan] for a fully Bayesian estimation of the model parameters and inference on hidden quantities, namely filtered state belief, smoothed state belief, jointly most probable state path and fitted output. The model is introduced only briefly, a more detailed mathematical treatment can be found in our [literature review](../litreview/main.pdf).

### Acknowledgements

The authors acknowledge Google for financial support via the Google Summer of Code 2017.

---

# The Input-Output Hidden Markov Model

The IOHMM is an architecture proposed by @bengio1995input to map input sequences, sometimes called the control signal, to output sequences. It is a probabilistic framework that can deal with general sequence processing tasks such as production, classification, or prediction. It differs from HMM, which is part of the unsupervised learning paradigm, since it is capable of learning the output sequence itself instead of learning only the output sequence distribution.

## Definitions

As with HMM, IOHMM involves two interconnected models,

\begin{align*}
z_{t} &= f(z_{t-1}, \mat{u}_{t}) \\
\mat{x}_{t} &= g(z_{t  }, \mat{u}_{t}).
\end{align*}

The first line corresponds to the state model, which consists of discrete-time, discrete-state hidden states $z_t \in \{1, \dots, K\}$ whose transition depends on the previous hidden state $z_{t-1}$ and the input vector $\mat{u}_{t} \in \RR^M$. Additionally, the observation model is governed by $g(z_{t}, \mat{u}_{t})$, where $\mat{x}_t \in \RR^R$ is the vector of observations, emissions or output. The corresponding joint distribution is

\[
p(\mat{z}_{1:T}, \mat{x}_{1:T} | \mat{u}_{t}).
\]

In the proposed parametrization with continuous inputs and outputs, the state model involves a multinomial regression whose parameters depend on the previous state taking the value $i$,

\[
p(z_t | \mat{x}_{t}, \mat{u}_{t}, z_{t-1} = i) = \text{softmax}^{-1}(\mat{w}_i \mat{u}_{t}),
\]

and the observation model is built upon the Gaussian density with parameters depending on the current state taking the value $j$,

\[
p(\mat{x}_t | \mat{u}_{t}, z_{t} = j) = \mathcal{N}(\mat{x}_t | \mat{b}_j \mat{u}_t, \mat{\Sigma}_j).
\]

IOHMM adapts the logics of HMM to allow for input and output vectors, retaining its fully probabilistic quality. Hidden states are assumed to follow a multinomial distribution that depends on the input sequence. The transition probabilities $\Psi_t(i, j) = p(z_t = j | z_{t-1} = i, \mat{u}_{t})$, which govern the state dynamics, are driven by the control signal as well.

As for the output sequence, the local evidence at time $t$ now becomes $\psi_t(j) = p(\mat{x}_t | z_t = j, \mat{\eta}_t)$, where $\mat{\eta}_t = \ev{\mat{x}_t | z_t, \mat{u}_t}$ can be interpreted as the expected location parameter for the probability distribution of the emission $\mat{x}_{t}$ conditional on the input vector $\mat{u}_t$ and the hidden state $z_t$.

## Inference
There are several quantities of interest to be inferred from different algorithms. In this section, the discussion assumes that model parameters $\mat{\theta}$ are known.

### Filtering

A filter infers the belief state at a given step based on all the information available up to that point,

\begin{align*}
\alpha_t(j)
  & \triangleq p(z_t = j | \mat{x}_{1:t}, \mat{u}_{1:t}).
\end{align*}

It achieves better noise reduction than simply estimating the hidden state based on the current estimate $p(z_t | \mat{x}_{t})$. The filtering process can be run online, or recursively, as new data streams in. Filtered maginals can be computed recursively by means of the forward algorithm [@baum1967inequality].

### Smoothing

A smoother infers the belief state at a given state based on all the observations or evidence,

\[
\begin{align*}
\gamma_t(j)
  & \triangleq p(z_t = j | \mat{x}_{1:T}, \mat{u}_{1:T}) \\
  & \propto \alpha_t(j) \beta_t(j),
\end{align*}
\]

where

\begin{align*}
\beta_{t-1}(i)
  & \triangleq p(\mat{x}_{t:T} | z_{t-1} = i, \mat{u}_{t:T}).
\end{align*}

Although noise and uncertainty are significantly reduced as a result of conditioning on past and future data, the smoothing process can only be run offline. Inference can be done by means of the forwards-backwards algorithm, also know as the Baum-Welch algorithm [@baum1967inequality, @baum1970maximization].

### Most likely hidden path
It is also of interest to compute the most probable state sequence or path,

\[
\mat{z}^* = \argmax_{\mat{z}_{1:T}} p(\mat{z}_{1:T} | \mat{x}_{1:T}).
\]

The jointly most probable sequence of states can be inferred by means of maximum a posterior (MAP) estimation or Viterbi decoding.

## Parameter estimation
The parameters of the models are $\mat{\theta} = (\mat{\pi}_1, \mat{\theta}_h, \mat{\theta}_o)$, where $\mat{\pi}_1$ is the initial state distribution, $\mat{\theta}_h$ are the parameters of the hidden model and $\mat{\theta}_o$ are the parameters of the state-conditional density function $p(\mat{x}_t | z_t = j, \mat{u}_t)$. The form of $\mat{\theta}_h$ and $\mat{\theta}_o$ depend on the specification of the model. State transition may be characterized by a logistic or multinomial regression with parameters $\mat{w}_k$ for $k \in \{1, \dots, K\}$, while emissions may be modelled with with a linear regression with Gaussian error with parameters $\mat{b}_k$ and $\mat{\Sigma}_k$ for $k \in \{1, \dots, K\}$.

---

# Learning by simulation
We first create a simulation routine that generates data complying with the model assumptions and we then write a MCMC sampler in Stan to recover the parameters and other hidden quantities. Next, we feed our model with the real data used in the original work and analyse the results.

We believe that learning by simulation has many benefits, including:

* The possibility to confirm that the sampler works as intended, i.e. it retrieves the parameters used to generate the data.
* The generated data meets all the assumptions and the estimates are free of the effects of data contamination due to unmodeled phenomena, thus simplifying the interpretation of the results.
* The user can gain insight into the model functioning by trying different combinations of the parameters, inputs and outputs.

## Auxiliary files

### Math functions

We write an auxiliary R function to compute the softmax transformation of a given vector. The calculations are run in log scale for greater numerical stability, i.e. to avoid any overflow.

```{r}
source('../common/R/math.R')
```

### Plot functions

As plots are extensively used, we arrange the code in an auxiliary file.

```{r}
source('../common/R/plots.R')
```

### Simulation functions

We arrange the code to generate simulated data, as explained below, in an auxiliary file.

```{r}
source('../iohmm-sim/R/iohmm-sim.R')
```

## Generative model

We first write an R function for our generative model. The arguments are the sequence length $T$, the number of discrete hidden states $K$, the input matrix $\mat{u}$, the initial state distribution vector $\mat{\pi}_1$, a matrix wit the regressor of the multinomial regression that rules the hidden states dynamics $\mat{w}$, the name of a function drawing samples from the observation distribution and its parameters.

```{r}
iohmm_sim <- function(T, K, u, w, p.init, obs.model, b, s) {
  m <- ncol(u)

  if (dim(u)[1] != T)
    stop("The input matrix must have T rows.")

  if (any(dim(w) != c(K, m)))
    stop("The transition weight matrix must be of size Kxm, where m is the size of the input vector.")

  if (any(dim(b) != c(K, m)))
    stop("The regressors matrix must be of size Kxm, where m is the size of the input vector.")

  if (length(p.init) != K)
    stop("The vector p.init must have length K.")

  p.mat <- matrix(0, nrow = T, ncol = K)
  p.mat[1, ] <- p.init

  z <- vector("numeric", T)
  z[1] <- sample(x = 1:K, size = 1, replace = FALSE, prob = p.init)
  for (t in 2:T) {
    p.mat[t, ] <- softmax(sapply(1:K, function(j) {u[t, ] %*% w[j, ]}))
    z[t] <- sample(x = 1:K, size = 1, replace = FALSE, prob = p.mat[t, ])
  }

  x <- do.call(obs.model, list(u, z, b, s))

  list(
    u = u,
    z = z,
    x = x,
    p.mat = p.mat
  )
}
```

The initial hidden state is drawn from a multinomial distribution with one trial and event probabilities given by the initial state probability vector $\mat{\pi}_1$. The transition probabilities for each of the following steps $t$ are computed from a multinomial linear regression with vector parameters $\mat{w}_k$, one set per possible hidden state $k = 1, \dots, K$, and covariates $\mat{u}_t$. The hidden states are subsequently sampled based on the transition probabilities.

The observation for each step generates from a linear regressions with regressors $\mat{b}_k$ and error variance $\mat{\Sigma}_k$, one set per possible hidden state.

```{r}
obsmodel_reg <- function(...) {
  args <- list(...)
  u <- args$u
  z <- args$z
  b <- args$b
  s <- args$s

  K <- length(unique(z))
  m <- ncol(u)

  if (any(dim(b) != c(K, m)))
    stop("The regressors matrix must be of size Kxm, where m is the size of the input vector.")

  T.length <- nrow(u)

  x <- vector("numeric", T.length)
  for (t in 1:T.length) {
    x[t] <- rnorm(1, u[t, ] %*% b[z[t], ], s[z[t]])
  }

  return(x)
}
```

Alternatively, the observation could be generated from a Gaussian mixture density where $\lambda_{kl}$ is the component weight for the $l$-th component in the $k$-th state with $0 \le \lambda_{kl} \le 1 \ \forall \ l \in \{1, \dots, L\}, k \in \{1, \dots, K\}$ and $\sum_{l=1}^{L}{\lambda_{kl}} = 1 \ \forall \ k$.

```{r}
obsmodel_mix <- function(...) {
  args <- list(...)
  z <- args$z
  lambda <- args$lambda
  mu <- args$mu
  s <- args$s

  if (!all.equal(length(unique(z)), length(lambda), length(mu), length(s)))
    stop("The size of the vector parameters lambda, mu and s must equal to the
         number of different states.")

  T.length <- length(z)
  L <- ncol(lambda)

  x <- vector("numeric", T.length)
  for (t in 1:T.length) {
    l <- sample(1:L, 1, FALSE, prob = lambda[z[t], ])
    x[t] <- rnorm(1, mu[z[t], l], s[z[t], l])
  }

  return(x)
}
```

We set up our simulated experiments for a regression observation model with arbitrary values for all the involved parameters. Additionally, we define the settings for the Markov Chain Monte Carlo sampler.

```{r}
# Set up ------------------------------------------------------------------

# Data
T.length = 300
K = 3
M = 4
R = 1
u.intercept = FALSE
w = matrix(
  c(1.2, 0.5, 0.3, 0.1, 0.5, 1.2, 0.3, 0.1, 0.5, 0.1, 1.2, 0.1),
  nrow = K, ncol = M, byrow = TRUE)
b = matrix(
  c(5, 6, 7, 0.5, 1, 5, 0.01, -0.5, 0.01, -1, -5, 0.2),
  nrow = K, ncol = M, byrow = TRUE)
s = c(0.25, 1, 2.5)
p1 = c(0.45, 0.10, 0.45)

# Markov Chain Monte Carlo
n.iter = 500
n.warmup = 250
n.chains = 1
n.cores = 1
n.thin = 1
n.seed = 9000
```

It is worth noting that we decide to rely on only one chain to avoid between-chain label switching of regression parameters ^[Enlarge why]. We refer the reader to @betancourt2017identifying for an in-depth treatment of the diagnostics, causes and possible solutions for label switching in Bayesian Mixture Models.

We create random inputs from a standard Gaussian distribution and generate the dataset accordingly.

```{r}
# Data simulation ---------------------------------------------------------
set.seed(n.seed)
u <- matrix(rnorm(T.length*M, 0, 1), nrow = T.length, ncol = M)
if (u.intercept)
  u[, 1] = 1

dataset <- iohmm_sim(T.length, K, u, w, p1, obs.model, b, s)
```

```{r, fig.height = 5, fig.width = 7.2, out.width="\\textwidth"}
plot_inputoutput(x = dataset$x, u = dataset$u, z = dataset$z)
```

We observe how the chosen values for the parameters affect the generated data. For example, the relationship between the third input $\mat{u}_3$ and the output $\mat{x}$ is positive, indifferent and negative for the hidden states 1 to 3, the true slopes being `r b[1, 3]`, `r b[2, 3]` and `r b[3, 3]` respectively.

```{r, fig.height = 6, fig.width = 7.2, out.width="\\textwidth"}
plot_inputprob(u = dataset$u, p.mat = dataset$p.mat, z = dataset$z)
```

We then analyse the relationship between the input and the state probabilities, which are usually hidden in applications with real data. We observe the strongest relationships between the pairs $\mat{u}_1, p(z_t = 1)$, $\mat{u}_2, p(z_t = 2)$ and $\mat{u}_3, p(z_t = 3)$, which is unsurprising given the values of the true regression parameters: those inputs take the largest weight in each state, namely $w_{11} = `r w[1, 1]`$, $w_{22} = `r w[2, 2]`}$ and $w_{33} = `r w[3, 3]`$.

Using a fully Bayesian approach, we now rely on our MCMC sampler to draw samples from the posterior density of model parameters and other hidden quantities ^[The Stan code is treated as given by now and will be explained below].

```{r}
# Model estimation --------------------------------------------------------
library(rstan)
library(shinystan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

stan.model = '../iohmm-reg/stan/iohmm-reg.stan'
stan.data = list(
  T = T.length,
  K = K,
  M = M,
  u_tm = as.array(u),
  x_t = dataset$x
)

stan.fit <- stan(file = stan.model,
                 data = stan.data, verbose = T,
                 iter = n.iter, warmup = n.warmup,
                 thin = n.thin, chains = n.chains,
                 cores = n.cores, seed = n.seed)

n.samples = (n.iter - n.warmup) * n.chains
```

We rely on the Rhat statistics and several diagnostic plots provided by shinystan^[cite] to assess mixing, convergence and the inexistence of divergences. We extract the samples for some quantities of interest, namely the filtered probabilities $\mat{alpha}_t$, the smoothed probability vector $\mat{\gamma}_t$, the most probable hidden path $\mat{z}^*$ and the fitted output $\hat{x}$.

```{r}
# MCMC Diagnostics --------------------------------------------------------
options(digits = 2)
summary(stan.fit,
        pars = c('p_1k', 'w_km', 'b_km', 's_k'),
        probs = c(0.50))$summary
# launch_shinystan(stan.fit)

# Extraction --------------------------------------------------------------
alpha_tk <- extract(stan.fit, pars = 'alpha_tk')[[1]]
gamma_tk <- extract(stan.fit, pars = 'gamma_tk')[[1]]
zstar_t <- extract(stan.fit, pars = 'zstar_t')[[1]]
hatx_t  <- extract(stan.fit, pars = 'hatx_t')[[1]]
```

While mixing and convergence is very good, which is expected as we are dealing with data generated exactly as assumed by the model, we note that regression parameters for latent states perform worse than other parameters. The small effective size translates into a high Monte Carlo standard error and broader intervals. One possible reason is that softmax is invariant to change in location, thus the parameters of a multinomial regression do not have a natural center and become harder to estimate.

We want to assess if the hidden states were recovered correctly by our software. Unfortunately, due to the label switching problem, the states generated under the labels 1 to 3 were recovered in inverse order. In consequence, we decide to relabel the data based on best fit. This would not prove to be a problem with real data since the hidden states are never observed.

```{r}
# Relabelling (ugly hack edition) -----------------------------------------
dataset$zrelab <- rep(0, T)

hard <- sapply(1:T.length, function(t, med) {
  which.max(med[t, ])
}, med = apply(alpha_tk, c(2, 3),
                    function(x) {
                      quantile(x, c(0.50)) }))

tab <- table(hard = hard, original = dataset$z)

for (k in 1:K) {
  dataset$zrelab[which(dataset$z == k)] <- which.max(tab[, k])
}

print("Label re-imputation (relabeling due to switching labels)")
table(new = dataset$zrelab, original = dataset$z)
```

Point estimates and credibility intervals are provided by rstan's^[cite] `{r}summary` function.

```{r}
# Estimation summary ------------------------------------------------------
print("Estimated initial state probabilities")
summary(stan.fit,
        pars = c('p_1k'),
        probs = c(0.10, 0.50, 0.90))$summary[, c(1, 3, 4, 5, 6)]

print("Estimated probabilities in the transition matrix")
head(summary(stan.fit,
        pars = c('A_ij'),
        probs = c(0.10, 0.50, 0.90))$summary[, c(1, 3, 4, 5, 6)])

print("Estimated regressors of hidden states")
summary(stan.fit,
        pars = c('w_km'),
        probs = c(0.10, 0.50, 0.90))$summary[, c(1, 3, 4, 5, 6)]

print("Estimated regressors and standard deviation of observations in each state")
summary(stan.fit,
        pars = c('b_km', 's_k'),
        probs = c(0.10, 0.50, 0.90))$summary[, c(1, 3, 4, 5, 6)]
```

We observed that the samples drawn from the posterior density are reasonably close to the true values. We additionally check the estimated hidden quantities.

```{r, fig.height = 6, fig.width = 7.2, out.width="\\textwidth"}
# Inference summary -------------------------------------------------------
# Filtered and smoothed state probability plot
plot_stateprobability(alpha_tk, gamma_tk, 0.8, dataset$zrelab)

# Confusion matrix for hard (naive) classification
print("Estimated hidden states (hard naive classification using filtered prob)")
print(table(
  estimated = apply(round(apply(alpha_tk, c(2, 3),
                                function(x) {
                                  quantile(x, c(0.50)) })), 1, which.max),
  real = dataset$zrelab))
```

The filtered probability is overly accurate, as expected, making the additional information contained in the smoothed seem of little value. The confusion matrix makes it clear that, under ideal conditions, the sampler and the model works as intended.

```{r, fig.height = 6, fig.width = 7.2, out.width="\\textwidth"}
# Jointly most likely state path (Viterbi decoding)
plot_statepath(zstar_t, dataset$zrelab)

# Confusion matrix for jointly most likely state path
print("Estimated hidden states for the jointly most likely path (Viterbi decoding)")
round(table(
  actual = rep(dataset$zrelab, each = n.samples),
  fit = zstar_t) / n.samples, 0)
```

The most probably hidden state path, computed by the Viterbi algorithm, . Finally, the fitted output is ...

```{r, fig.height = 6, fig.width = 7.2, out.width="\\textwidth"}
# Fitted output
plot_outputfit(dataset$x, hatx_t, z = dataset$zrelab, TRUE)
```


All in all, we are confindent that the Stan code works as specified and we find model calibration satisfactory.

---

# Stock Market Forecasting Using Hidden Markov Model

## Model

We adhere to the methodology proposed in the original work as much as possible. We set up an IOHMM model with $K = 4$ hidden states to forecast stock prices. The hidden state model is driven by a multinomial (softmax) regression with $R = 4$ inputs, namely opening, closing, highest and lowest prices from the previous time step. The univariate output is the stock closing price for the current time step. The observation model is a Gaussian mixture with $L = 3$ components per state, that is

\[
p(x_t | z_{t} = j) = \sum_{l = 1}^{L}{\lambda_{jl} \ \NN(x_t | \mu_{jl}, \sigma_{jl})},
\]

where $\lambda_{jl}$ is the component weight for the $l$-th component in the $j$-th state with $0 \le \lambda_{jl} \le 1 \ \forall \ l, j$ and $\sum_{l=1}^{L}{\lambda_{jl}} = 1 \ \forall \ j$.

The vector parameter then becomes $\mat{\theta} = (\mat{\pi}_1, \mat{w}_k, \lambda_{kl}, \mu_{kl}, \sigma_{kl})$ for $k \in \{1, \dots, K\}$ and $l \in \{1, \dots, L\}$.

## Dataset

We use daily OHLC prices for the stocks and time spans listed in the table below. The data is downloaded from Google Finance via Quantmod.

```{r}
symbols <- data.frame(
  symbol    = c("NYSE:DAL", "NYSE:DAL", "NYSE:LUV", "LON:RYA"),
  name      = c("BA", "Delta Air Lines, Inc.", "Southwest Airlines Co", "Ryanair Holdings Plc"),
  train.from = c("17-09-2002", "27-12-2002", "18-12-2002", "06-05-2003"),
  train.to   = c("10-09-2004", "31-08-2004", "23-07-2004", "06-12-2004"),
  test.from  = c("11-09-2004", "01-09-2004", "24-07-2004", "07-12-2004"),
  test.to    = c("20-01-2005", "17-11-2004", "17-11-2004", "17-03-2005"),
  stringsAsFactors = FALSE)

library(quantmod)
invisible(Sys.setlocale("LC_MESSAGES", "C"))  # Google vs date format fix
invisible(Sys.setlocale("LC_TIME", "C"))      # https://stackoverflow.com/a/20855453/2860744

get_prices <- function(symbol, train.from, train.to, test.from, test.to, ...) {
  prices <- getSymbols(symbol, env = NULL, from = train.from, to = test.to, ...)
  list(
    train = prices[paste(train.from, train.to, sep = "/")],
    test  = prices[paste(test.from, test.to, sep = "/")]
  )
}
```

```{r, echo = FALSE}
library(knitr)
kable(t(sapply(1:nrow(symbols), function(i) { c(
        symbols$symbol[i],
        symbols$name[i],
        paste(symbols$train.from[i], "to", symbols$train.to[i]),
        paste(symbols$test.from[i] , "to", symbols$test.to[i])
      )})),
      col.names = c("Symbol", "Name", "Training set", "Test set"),
      align = "c", caption = "Details of the dataset.")
```

## Methodology

The goal of the experiment is to predict the closing price for a specific stock market share. We estimate the model parameters $\mat{\hat{\theta}}$ using all the information avaliable in the training dataset. For each day $h$ in the training set, we compute the likelihood of the input vector conditional on the trained parameters $p(\mat{u}_{t+h} | \mat{\hat{\theta}})$ and we locate the observation in the training set with the nearest likelihood,

\[
\mat{u}_{t+h}^* = \argmin_{\mat{u}_t} \ \left| p(\mat{u}_{t} | \mat{\hat{\theta}}) - p(\mat{u}_{t+h} | \mat{\hat{\theta}}) \right|.
% $\hat{\LL}_{t+h} = p(\mat{u}_{t+h} | \mat{\hat{\theta}})$
% $\hat{\LL}_{t+h}^* = p(\mat{u}_{t+h}^* | \mat{\hat{\theta}})$ where 
\]

We compute the change in the closing price from the located time step to the next,

\[
\Delta_{t+h}^* = \mat{x}_{t+h}^* - \mat{x}_{t+h-1}^*},
\]

and build out forecast assuming that a similar pattern will repeat

\[
\hat{\mat{x}_{t+h}} = \hat{\mat{x}_{t+h-1}} + \Delta_{t+h}^*.
\]

The procedure is run independently for each stock.

## Programming code

```{r}
library(quantmod)
invisible(Sys.setlocale("LC_MESSAGES", "C"))  # Google vs date format fix
invisible(Sys.setlocale("LC_TIME", "C"))      # https://stackoverflow.com/a/20855453/2860744

iohmmmix_estimate <- function(symbol, ...) {
  prices <- getSymbols(symbol, env = NULL, src = "google", from = train.from, to = test.to)

  # Estimation data
  stan.model = 'stan/iohmm-mix.stan'
  stan.data = list(
    T = T.length,
    K = K,
    M = M,
    L = L,
    u_tm = as.array(u),
    x_t = dataset$x
  )

  stan.fit <- stan(file = stan.model,
                   data = stan.data, verbose = T,
                   iter = n.iter, warmup = n.warmup,
                   thin = n.thin, chains = n.chains,
                   cores = n.cores, seed = n.seed)
}

# i = 2
# symbol <- symbols$symbol[i]
# name <- symbols$name[i]
# train.from <- symbols$train.from[i]
# train.to <- symbols$train.to[i]
# test.from <- symbols$test.from[i]
# test.to <- symbols$test.to[i]
# 
# prices <- getSymbols(symbol, env = NULL, src = "google", from = train.from, to = train.to)

```



---

# Original Computing Environment

```{r}
writeLines(readLines(file.path(Sys.getenv("HOME"), ".R/Makevars")))
devtools::session_info("rstan")
```

---

# References
